{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Importing Libraries and Setup\n",
        "# -------------------------------------------\n",
        "\n",
        "# TensorFlow is a deep learning framework developed by Google.\n",
        "# It provides tools for building and training neural networks.\n",
        "import tensorflow as tf\n",
        "\n",
        "# The 'datasets', 'layers', and 'models' modules from Keras\n",
        "# (which is integrated into TensorFlow) are used to load data,\n",
        "# build CNN layers, and define sequential or functional models.\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "# Matplotlib is used for visualizing images, accuracy, and loss curves.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NumPy is a library for numerical operations and handling image data arrays.\n",
        "import numpy as np\n",
        "\n",
        "# Converts integer labels into one-hot encoded vectors (e.g., 3 â†’ [0,0,0,1,0,...])\n",
        "# which is essential for training classification models.\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# -------------------------------------------\n",
        "# Notes:\n",
        "# - Keras is built on top of TensorFlow and simplifies neural network creation.\n",
        "# - PyTorch can also be used for CNNs and large-scale architectures (e.g., LLMs).\n",
        "# - Unlike Keras, PyTorch handles parallelism more flexibly and natively.\n",
        "# -------------------------------------------\n",
        "\n",
        "\"\"\"\n",
        "ğŸ“˜ Summary:\n",
        "This cell imports all required libraries for building and training a CNN model using TensorFlow and Keras.\n",
        "It also prepares tools for visualization (matplotlib) and preprocessing (NumPy and to_categorical).\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "Ry_hGzWUS1lJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "502e5eb3-39f0-4168-c796-1d7a7762e980"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nğŸ“˜ Summary:\\nThis cell imports all required libraries for building and training a CNN model using TensorFlow and Keras.\\nIt also prepares tools for visualization (matplotlib) and preprocessing (NumPy and to_categorical).\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Loading and Preprocessing the MNIST Dataset\n",
        "# -------------------------------------------\n",
        "\n",
        "# Import the MNIST dataset (handwritten digits 0â€“9)\n",
        "# It comes pre-split into training and testing sets.\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Utility function to convert class labels into one-hot encoded form.\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# NumPy for numerical operations (array slicing, normalization, etc.)\n",
        "import numpy as np\n",
        "\n",
        "# TensorFlow backend (Keras is built on top of TensorFlow)\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# -------------------------------------------\n",
        "# Load Dataset\n",
        "# -------------------------------------------\n",
        "# The MNIST dataset contains 70,000 grayscale images (28x28 pixels each):\n",
        "# 60,000 for training and 10,000 for testing.\n",
        "# The function automatically splits the data into (features, labels).\n",
        "# Can load other datasets directly also like : datasets.cifar10.load_data()\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Example:\n",
        "# X_train.shape -> (60000, 28, 28)\n",
        "# y_train contains digit labels (0â€“9) corresponding to each image.\n",
        "\n",
        "\n",
        "# -------------------------------------------\n",
        "# Subset Selection (for faster training/demo)\n",
        "# -------------------------------------------\n",
        "\n",
        "subset_size = 5000  # use a smaller subset of data for quicker experimentation\n",
        "\n",
        "# Select the first 5,000 samples from the training data\n",
        "X_train = X_train[:subset_size]\n",
        "y_train = y_train[:subset_size]\n",
        "\n",
        "# Select 1,000 samples from the test data (5000 - 4000)\n",
        "X_test = X_test[:subset_size - 4000]\n",
        "y_test = y_test[:subset_size - 4000]\n",
        "\n",
        "\n",
        "# -------------------------------------------\n",
        "# Normalization\n",
        "# -------------------------------------------\n",
        "# Each pixel in the MNIST images has an intensity value between 0 and 255.\n",
        "# Neural networks train more efficiently when input values are small and consistent.\n",
        "# Dividing by 255.0 scales all pixel values to the range [0, 1].\n",
        "# This helps stabilize gradients and speeds up convergence during training.\n",
        "\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "ğŸ“˜ Summary:\n",
        "This cell loads the MNIST handwritten digit dataset, reduces its size for quicker training,\n",
        "and normalizes pixel values to a 0â€“1 range. Normalization ensures that the CNN trains\n",
        "efficiently by preventing large gradient updates.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "A062WW9pTEFj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "10d75c09-01d7-49f2-88c8-c6f507771ea8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nğŸ“˜ Summary:\\nThis cell loads the MNIST handwritten digit dataset, reduces its size for quicker training,\\nand normalizes pixel values to a 0â€“1 range. Normalization ensures that the CNN trains\\nefficiently by preventing large gradient updates.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Visualizing a Sample Image from the Dataset\n",
        "# -------------------------------------------\n",
        "\n",
        "# Import matplotlib for image visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Display the 10th image (index 9) from the training dataset\n",
        "# 'cmap=\"gray\"' renders it as a grayscale image instead of default RGB.\n",
        "plt.imshow(X_train[9], cmap='gray')\n",
        "\n",
        "# Show the corresponding label for the selected image\n",
        "plt.title(f\"Label: {y_train[9]}\")\n",
        "\n",
        "# Remove the x and y axis ticks for cleaner display\n",
        "plt.axis('off')\n",
        "\n",
        "# Render the image\n",
        "plt.show()\n",
        "\n",
        "# Note: The image might look slightly lighter/distorted because of normalization\n",
        "# since all pixel values are scaled between 0 and 1 instead of 0â€“255.\n",
        "# This is normal and does not affect the modelâ€™s performance.\n",
        "\"\"\"\n",
        "ğŸ“˜ Summary:\n",
        "This cell visualizes one sample image from the normalized MNIST dataset\n",
        "to confirm that the data has been loaded and processed correctly.\n",
        "Normalization can make images appear slightly faded, but they still\n",
        "contain all the necessary information for training a CNN.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "yMGE2LKDUXyV",
        "outputId": "9d9becfd-bb05-4ac8-9897-978da5430034"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nğŸ“˜ Summary:\\nThis cell visualizes one sample image from the normalized MNIST dataset\\nto confirm that the data has been loaded and processed correctly.\\nNormalization can make images appear slightly faded, but they still\\ncontain all the necessary information for training a CNN.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Checking the Shape of the Test Dataset\n",
        "# -------------------------------------------\n",
        "\n",
        "# Display the dimensions of the test data array\n",
        "# Expected output format: (num_samples, height, width)\n",
        "# Since MNIST images are 28x28 pixels, and we selected 1000 samples earlier,\n",
        "# the result should be something like (1000, 28, 28)\n",
        "X_test.shape\n",
        "\n",
        "\"\"\"\n",
        "ğŸ“˜ Summary:\n",
        "This line checks the shape of the test dataset to verify the number of samples\n",
        "and image dimensions after subsetting. It helps confirm that preprocessing and slicing\n",
        "were applied correctly before reshaping or feeding data into the CNN.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "KypOEKiNTEmM",
        "outputId": "3c0e7812-0bc2-431d-b437-e9b3aab790e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nğŸ“˜ Summary:\\nThis line checks the shape of the test dataset to verify the number of samples\\nand image dimensions after subsetting. It helps confirm that preprocessing and slicing\\nwere applied correctly before reshaping or feeding data into the CNN.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Checking the Shape of the Training Labels\n",
        "# -------------------------------------------\n",
        "\n",
        "# Display the dimensions of the training labels (y_train)\n",
        "# Each element in y_train corresponds to the class label (0â€“9)\n",
        "# for its matching image in X_train.\n",
        "# Expected output: (5000,) since we selected 5000 samples earlier.\n",
        "y_train.shape\n",
        "\n",
        "\"\"\"\n",
        "ğŸ“˜ Summary:\n",
        "This line checks the number of training labels to ensure they match the number\n",
        "of training images. It confirms that the dataset slicing was applied correctly\n",
        "and that each image still has its corresponding label before one-hot encoding.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "lEXpDwjBTGt2",
        "outputId": "907c1438-8162-4166-c592-a48482fd422b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nğŸ“˜ Summary:\\nThis line checks the number of training labels to ensure they match the number\\nof training images. It confirms that the dataset slicing was applied correctly\\nand that each image still has its corresponding label before one-hot encoding.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Converting Labels to One-Hot Encoded Format\n",
        "# -------------------------------------------\n",
        "\n",
        "# Each label in y_train and y_test currently represents a class index (0â€“9).\n",
        "# Neural networks often require these labels to be converted into one-hot encoded vectors\n",
        "# for use with the 'categorical_crossentropy' loss function.\n",
        "#\n",
        "# Example:\n",
        "# Label: 3  â†’  One-hot vector: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "# If you choose to use one-hot encoding, then the appropriate loss function would be 'categorical_crossentropy'.\n",
        "# If you choose *not* to use one-hot encoding, then the appropriate loss function would be 'sparse_categorical_crossentropy' instead.\n",
        "\n",
        "# Convert integer labels into one-hot encoded vectors with 10 classes\n",
        "# (Uncomment these lines to apply one-hot encoding)\n",
        "\n",
        "# y_train = to_categorical(y_train, 10)\n",
        "# y_test = to_categorical(y_test, 10)\n",
        "\n",
        "\"\"\"\n",
        "ğŸ“˜ Summary:\n",
        "This cell prepares the target labels for training by explaining how to convert\n",
        "integer class labels into one-hot encoded form. One-hot encoding is required\n",
        "when using 'categorical_crossentropy', while 'sparse_categorical_crossentropy'\n",
        "can be used directly with integer labels. Both achieve the same goal of\n",
        "computing class probabilities, but in slightly different input formats.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "QAeRsoaRM-rB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "dcc0e6d6-4a02-42da-d1a7-5b407d1ca458"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nğŸ“˜ Summary:\\nThis cell prepares the target labels for training by explaining how to convert\\ninteger class labels into one-hot encoded form. One-hot encoding is required\\nwhen using 'categorical_crossentropy', while 'sparse_categorical_crossentropy'\\ncan be used directly with integer labels. Both achieve the same goal of\\ncomputing class probabilities, but in slightly different input formats.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Building a Basic Artificial Neural Network (ANN)\n",
        "# -------------------------------------------\n",
        "\n",
        "# The Sequential API allows stacking layers one after another in a linear fashion.\n",
        "ann = models.Sequential([\n",
        "\n",
        "    # The Flatten layer converts each 28x28 image into a 1D vector of size 784 (28*28).\n",
        "    # This is necessary because dense (fully connected) layers accept 1D input only.\n",
        "    layers.Flatten(input_shape=(28, 28)),\n",
        "\n",
        "    # First hidden layer with 3000 neurons.\n",
        "    # ReLU (Rectified Linear Unit) activation introduces non-linearity\n",
        "    # and helps the network learn complex patterns.\n",
        "    layers.Dense(3000, activation='relu'),\n",
        "\n",
        "    # Second hidden layer with 1000 neurons, also using ReLU.\n",
        "    # Stacking dense layers deepens the network and improves feature learning.\n",
        "    layers.Dense(1000, activation='relu'),\n",
        "\n",
        "    # Output layer with 10 neurons, one for each MNIST digit (0â€“9).\n",
        "    # Softmax converts the outputs into probabilities that sum to 1.\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# -------------------------------------------\n",
        "# Compiling the Model\n",
        "# -------------------------------------------\n",
        "\n",
        "# The Adam optimizer combines momentum and adaptive learning rates\n",
        "# for efficient and fast convergence.\n",
        "# Since labels are integers (not one-hot encoded),\n",
        "# we use 'sparse_categorical_crossentropy' as the loss function.\n",
        "ann.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']  # Track accuracy during training and evaluation\n",
        ")\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "# including layer types, output shapes, and number of trainable parameters.\n",
        "ann.summary()\n",
        "\n",
        "\"\"\"\n",
        "ğŸ“˜ Summary:\n",
        "This cell builds and compiles a simple ANN for handwritten digit classification.\n",
        "The network flattens each image, passes it through two hidden layers with ReLU activation,\n",
        "and outputs class probabilities using Softmax. The model is trained using the Adam optimizer\n",
        "and sparse categorical cross-entropy loss because the labels are integer-encoded.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "Ih9RbEmrTWm9",
        "outputId": "b458bd7b-fa5d-409e-e060-3f943e854c56"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3000\u001b[0m)           â”‚     \u001b[38;5;34m2,355,000\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           â”‚     \u001b[38;5;34m3,001,000\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             â”‚        \u001b[38;5;34m10,010\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3000</span>)           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,355,000</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,001,000</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,010</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,366,010\u001b[0m (20.47 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,366,010</span> (20.47 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,366,010\u001b[0m (20.47 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,366,010</span> (20.47 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nğŸ“˜ Summary:\\nThis cell builds and compiles a simple ANN for handwritten digit classification.\\nThe network flattens each image, passes it through two hidden layers with ReLU activation,\\nand outputs class probabilities using Softmax. The model is trained using the Adam optimizer\\nand sparse categorical cross-entropy loss because the labels are integer-encoded.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Training the Artificial Neural Network (ANN)\n",
        "# -------------------------------------------\n",
        "\n",
        "# Train the ANN model using the training data for 3 epochs.\n",
        "# Each epoch means one full pass through the entire training dataset.\n",
        "# 'batch_size' determines how many samples are processed before updating the model weights.\n",
        "ann.fit(X_train, y_train, epochs=3, batch_size=32)\n",
        "\n",
        "# -------------------------------------------\n",
        "# Notes on Batch Size and Iterations\n",
        "# -------------------------------------------\n",
        "\n",
        "# Number of iterations (steps per epoch) = Total training samples / batch_size\n",
        "# For example, 5000 samples / 32 â‰ˆ 157 iterations per epoch.\n",
        "\n",
        "# Smaller batch size:\n",
        "# - More weight updates â†’ captures fine-grained/local patterns.\n",
        "# - Slower training but may lead to better generalization.\n",
        "\n",
        "# Larger batch size:\n",
        "# - Fewer updates per epoch â†’ faster training.\n",
        "# - May lose some critical local variations in data.\n",
        "\n",
        "\"\"\"\n",
        "ğŸ“˜ Summary:\n",
        "This cell trains the ANN for 3 epochs using mini-batches of 32 images each.\n",
        "The batch size controls how frequently model weights are updated during training.\n",
        "Smaller batches increase iteration count and improve sensitivity to detail,\n",
        "while larger batches speed up training but may reduce fine-grained learning.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "E0gc5dXXY5Ed",
        "outputId": "04544e86-879c-4d32-f22f-16d08d41fdb9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m157/157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 76ms/step - accuracy: 0.7774 - loss: 0.6910\n",
            "Epoch 2/3\n",
            "\u001b[1m157/157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 72ms/step - accuracy: 0.9518 - loss: 0.1541\n",
            "Epoch 3/3\n",
            "\u001b[1m157/157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 74ms/step - accuracy: 0.9753 - loss: 0.0770\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nğŸ“˜ Summary:\\nThis cell trains the ANN for 3 epochs using mini-batches of 32 images each.\\nThe batch size controls how frequently model weights are updated during training.\\nSmaller batches increase iteration count and improve sensitivity to detail,\\nwhile larger batches speed up training but may reduce fine-grained learning.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Evaluating Model Performance\n",
        "# -------------------------------------------\n",
        "\n",
        "# Import performance evaluation tools from scikit-learn\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Use the trained ANN model to make predictions on the test set\n",
        "# The output (y_pred) will contain probability distributions for each of the 10 classes.\n",
        "y_pred = ann.predict(X_test)\n",
        "\n",
        "# Convert predicted probability vectors into class indices (0â€“9)\n",
        "# np.argmax() selects the index of the maximum probability in each prediction.\n",
        "y_pred_classes = [np.argmax(element) for element in y_pred]\n",
        "\n",
        "# Display a detailed classification report\n",
        "# Includes precision, recall, F1-score, and support for each class.\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_classes))\n",
        "\n",
        "\"\"\"\n",
        "ğŸ“˜ Summary:\n",
        "This cell evaluates the trained ANN using the test dataset.\n",
        "Predicted probabilities are converted into class labels using argmax.\n",
        "The classification report summarizes model performance across all 10 digits\n",
        "by showing metrics like precision (accuracy per class), recall (sensitivity),\n",
        "and F1-score (balance between precision and recall).\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "tq_W6grATYEk",
        "outputId": "07d00e7c-adac-450d-d82c-69a21b1293bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98        85\n",
            "           1       0.97      1.00      0.98       126\n",
            "           2       0.93      0.96      0.94       116\n",
            "           3       0.82      0.93      0.87       107\n",
            "           4       0.93      0.87      0.90       110\n",
            "           5       0.91      0.93      0.92        87\n",
            "           6       0.94      0.94      0.94        87\n",
            "           7       0.97      0.70      0.81        99\n",
            "           8       0.88      0.91      0.90        89\n",
            "           9       0.86      0.91      0.89        94\n",
            "\n",
            "    accuracy                           0.92      1000\n",
            "   macro avg       0.92      0.91      0.91      1000\n",
            "weighted avg       0.92      0.92      0.91      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nğŸ“˜ Summary:\\nThis cell evaluates the trained ANN using the test dataset.\\nPredicted probabilities are converted into class labels using argmax.\\nThe classification report summarizes model performance across all 10 digits\\nby showing metrics like precision (accuracy per class), recall (sensitivity),\\nand F1-score (balance between precision and recall).\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Building a Convolutional Neural Network (CNN)\n",
        "# -------------------------------------------\n",
        "\n",
        "# Sequential API: stack layers in a linear order (input â†’ output)\n",
        "cnn = models.Sequential([\n",
        "\n",
        "    # 1ï¸âƒ£ First Convolutional Layer\n",
        "    # Applies 32 filters (kernels) of size 3x3 to the input image.\n",
        "    # Each kernel slides across the image to detect local patterns (edges, curves, textures).\n",
        "    # 'activation=\"relu\"' introduces non-linearity.\n",
        "    # Input shape is (28, 28, 1) â€” 28x28 grayscale image with 1 channel.\n",
        "    # The kernel weights are initialized randomly and learned during training.\n",
        "    layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "\n",
        "    # 2ï¸âƒ£ Max Pooling Layer\n",
        "    # Reduces spatial dimensions (downsamples) by taking the max value in each 2x2 region.\n",
        "    # Helps extract dominant features and reduces computation.\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # 3ï¸âƒ£ Second Convolutional Layer\n",
        "    # Adds more filters (64) to learn more complex patterns and higher-level features.\n",
        "    # Kernel size again is 3x3, with ReLU activation.\n",
        "    # Can optionally specify stride or padding here (e.g., padding='same').\n",
        "    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
        "\n",
        "    # 4ï¸âƒ£ Second Max Pooling Layer\n",
        "    # Further reduces the feature map size to make learning efficient.\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # 5ï¸âƒ£ Flatten Layer\n",
        "    # Converts the 2D feature maps into a 1D vector.\n",
        "    # This step is required before feeding into fully connected (dense) layers.\n",
        "    layers.Flatten(),\n",
        "\n",
        "    # 6ï¸âƒ£ Fully Connected (Dense) Layer\n",
        "    # 64 neurons with ReLU activation â€” combines extracted features to form class-level patterns.\n",
        "    layers.Dense(64, activation='relu'),\n",
        "\n",
        "    # 7ï¸âƒ£ Output Layer\n",
        "    # 10 neurons (one for each digit 0â€“9), using softmax to output class probabilities.\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\"\"\"\n",
        "ğŸ“˜ Summary:\n",
        "This CNN model processes input images through a hierarchy of convolution and pooling layers\n",
        "to automatically extract spatial features. After flattening, dense layers perform classification.\n",
        "Key elements:\n",
        "- Conv2D layers learn local features via sliding kernels.\n",
        "- MaxPooling2D layers downsample to reduce complexity.\n",
        "- Flatten + Dense layers handle final classification.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "lzyapbLtTYQD",
        "outputId": "27e36148-03f2-4b57-e95d-5534b93b7c4a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nğŸ“˜ Summary:\\nThis CNN model processes input images through a hierarchy of convolution and pooling layers\\nto automatically extract spatial features. After flattening, dense layers perform classification.\\nKey elements:\\n- Conv2D layers learn local features via sliding kernels.\\n- MaxPooling2D layers downsample to reduce complexity.\\n- Flatten + Dense layers handle final classification.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sigmoid\n",
        "0.4\n",
        "0.5\n",
        "0.6"
      ],
      "metadata": {
        "id": "-CcN4fjZgTuF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc83a6d-bc13-4daf-9588-23f94ff8b75a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Softmax\n",
        "(0.4)/(0.4+0.5+0.6)\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------------------------\n",
        "# Understanding Sigmoid vs Softmax Activation\n",
        "# -------------------------------------------\n",
        "\n",
        "# Sigmoid: Each output neuron acts independently and outputs a value between 0 and 1.\n",
        "# Example:\n",
        "# For logits [0.4, 0.5, 0.6], sigmoid would be applied individually to each.\n",
        "\n",
        "# Softmax: Converts all logits into a probability distribution over classes.\n",
        "# Example:\n",
        "# For raw values [0.4, 0.5, 0.6], softmax normalizes them:\n",
        "# 0.4 / (0.4 + 0.5 + 0.6), etc.\n",
        "# The outputs sum to 1.0 and represent probabilities for multi-class prediction.\n",
        "\n",
        "\"\"\"\n",
        "ğŸ“˜ Summary:\n",
        "This snippet conceptually demonstrates how sigmoid handles independent probabilities\n",
        "for each neuron, while softmax distributes probability across all classes so that\n",
        "the sum equals 1. It helps students understand why softmax is used for classification layers.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "fx4JDIlrh3kK",
        "outputId": "32cb0cb8-ddd8-4045-e0c2-5f4aee4f0f42"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nğŸ“˜ Summary:\\nThis snippet conceptually demonstrates how sigmoid handles independent probabilities\\nfor each neuron, while softmax distributes probability across all classes so that\\nthe sum equals 1. It helps students understand why softmax is used for classification layers.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Compiling the Convolutional Neural Network\n",
        "# -------------------------------------------\n",
        "\n",
        "# The compile() method configures the model for training.\n",
        "cnn.compile(\n",
        "    optimizer='adam',                     # Adam optimizer adapts learning rates automatically for each parameter.\n",
        "    loss='sparse_categorical_crossentropy', # Used when labels are integer-encoded (not one-hot).\n",
        "    metrics=['accuracy']                  # Tracks accuracy during training and evaluation.\n",
        ")\n",
        "\n",
        "# Notes:\n",
        "# - If labels were one-hot encoded, weâ€™d use 'categorical_crossentropy' instead.\n",
        "# - The optimizer, loss, and metrics determine how the model learns and how performance is measured.\n",
        "\n",
        "\"\"\"\n",
        "ğŸ“˜ Summary:\n",
        "This cell compiles the CNN model by specifying how it should learn.\n",
        "It uses the Adam optimizer for efficient gradient updates,\n",
        "the sparse categorical cross-entropy loss for integer-based class labels,\n",
        "and accuracy as the performance metric.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "PmjUhw9RTYam",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "cc2aecaa-cfd9-4593-bf7b-15415c938e04"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nğŸ“˜ Summary:\\nThis cell compiles the CNN model by specifying how it should learn.\\nIt uses the Adam optimizer for efficient gradient updates,\\nthe sparse categorical cross-entropy loss for integer-based class labels,\\nand accuracy as the performance metric.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.fit(X_train, y_train, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgDHcHO8Tje-",
        "outputId": "786da888-3c5e-40b8-ddaa-695eba09bee3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m157/157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.6237 - loss: 1.2363\n",
            "Epoch 2/3\n",
            "\u001b[1m157/157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.9528 - loss: 0.1742\n",
            "Epoch 3/3\n",
            "\u001b[1m157/157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - accuracy: 0.9671 - loss: 0.1084\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7eb94054ac60>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN's are best for image classification and gives superb accuracy. As, at the end 3 of epochs,\n",
        "\n",
        "1.   Accuracy was at around 96% - a significant improvement over ANN\n",
        "2.   Also, Extremly less computation compared to simple ANN\n",
        "1.   As, Maxpooling reduces the image dimensions while still preserving the features.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pxE_6t2oT2Nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Evaluating the CNN on Test Data\n",
        "# -------------------------------------------\n",
        "\n",
        "# Evaluate the trained CNN model on unseen test data.\n",
        "# This method returns two values:\n",
        "# 1ï¸âƒ£ Test loss  â†’ how well the model fits the data (lower is better)\n",
        "# 2ï¸âƒ£ Test accuracy â†’ how many predictions were correct (higher is better)\n",
        "cnn.evaluate(X_test, y_test)\n",
        "\n",
        "# The evaluation helps determine how well the model generalizes\n",
        "# beyond the training dataset, revealing potential overfitting or underfitting.\n",
        "\n",
        "\"\"\"\n",
        "ğŸ“˜ Summary:\n",
        "This cell tests the trained CNN on the test dataset to measure its performance.\n",
        "The evaluate() function returns loss and accuracy metrics,\n",
        "which indicate how well the CNN can classify new, unseen images.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "8XkbNomqUvt0",
        "outputId": "4095a105-6b53-493f-e5fb-b226ecb64235"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9747 - loss: 0.0834\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nğŸ“˜ Summary:\\nThis cell tests the trained CNN on the test dataset to measure its performance.\\nThe evaluate() function returns loss and accuracy metrics,\\nwhich indicate how well the CNN can classify new, unseen images.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}