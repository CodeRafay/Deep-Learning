{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEvP3aQxu22u"
      },
      "outputs": [],
      "source": [
        "!rm -f data.csv\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Opens a file picker in Google Colab so you can upload files from your computer\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Loop through all uploaded files\n",
        "for fn in uploaded.keys():\n",
        "    # Print the filename and its size in bytes\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])\n",
        "    ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwZ4KEa3yigy"
      },
      "outputs": [],
      "source": [
        "# Display the dictionary of uploaded files (from files.upload earlier)\n",
        "uploaded\n",
        "\n",
        "# Import pandas for data handling and io for reading file content\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# Read the uploaded CSV file into a pandas DataFrame\n",
        "# - uploaded['data.csv'] accesses the uploaded file content (in bytes)\n",
        "# - .decode('utf-8') converts the bytes into a string\n",
        "# - io.StringIO(...) makes the string behave like a file\n",
        "# - pd.read_csv(...) loads the CSV into a structured DataFrame\n",
        "data = pd.read_csv(io.StringIO(uploaded['data.csv'].decode('utf-8')))\n",
        "\n",
        "# Display the first 5 rows of the dataset to verify it loaded correctly\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVB-OKG2-iEJ"
      },
      "outputs": [],
      "source": [
        "# Importing seaborn for data visualization\n",
        "import seaborn as sns\n",
        "\n",
        "# Creates a count plot (bar chart) of the 'diagnosis' column from the dataframe 'data'\n",
        "# It shows how many times each category (Benign or Malignant) appears\n",
        "ax = sns.countplot(data['diagnosis'], label='Count')\n",
        "\n",
        "# Unpacks the counts of each diagnosis category into variables B and M\n",
        "# data['diagnosis'].value_counts() returns the frequency of each class\n",
        "B, M = data['diagnosis'].value_counts()\n",
        "\n",
        "# Prints the count of Benign cases\n",
        "print('Benign', B)\n",
        "\n",
        "# Prints the count of Malignant cases\n",
        "print('Malignant', M)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYY5UexY-mbk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45TrFVGg-m1x"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Splitting dataset into Features (X) and Labels (y)\n",
        "# -----------------------------\n",
        "\n",
        "# X: selecting all rows, and all columns from index 2 onwards (skips 'id' and 'diagnosis').\n",
        "#    So X contains only the numeric feature columns such as radius_mean, texture_mean, etc.\n",
        "X = data.iloc[:, 2:].values\n",
        "\n",
        "# y: selecting the diagnosis column (index 1).\n",
        "#    This contains 'M' (Malignant) or 'B' (Benign) for each patient.\n",
        "y = data.iloc[:, 1].values\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Encoding categorical labels\n",
        "# -----------------------------\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Creating a LabelEncoder object to convert text labels into numbers\n",
        "labelencoder_X_1 = LabelEncoder()\n",
        "\n",
        "# Transform 'M' and 'B' into numeric values:\n",
        "#   'M' → 1 (Malignant)\n",
        "#   'B' → 0 (Benign)\n",
        "y = labelencoder_X_1.fit_transform(y)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Splitting the dataset into Training and Test sets\n",
        "# -----------------------------\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Divide dataset into training and test data:\n",
        "#   X_train, y_train → used to train the model (80% of data)\n",
        "#   X_test, y_test   → used to test the model (20% of data)\n",
        "# random_state=0 ensures the split is reproducible\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Feature Scaling\n",
        "# -----------------------------\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Creating a StandardScaler object to normalize features\n",
        "sc = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform it:\n",
        "# Each feature will have mean=0 and standard deviation=1.\n",
        "# This ensures that features like 'area_mean' (which can be large, e.g. 1000s)\n",
        "# don't dominate smaller features like 'smoothness_mean'.\n",
        "X_train = sc.fit_transform(X_train)\n",
        "\n",
        "# Apply the same scaling transformation on test data\n",
        "# (Important: we do not fit again on test data, to avoid data leakage).\n",
        "X_test = sc.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDLtdvuk-nAT"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hk217U6t_0Ll"
      },
      "outputs": [],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bKxLHZo_2kl"
      },
      "outputs": [],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvI1fy4YCKnG"
      },
      "outputs": [],
      "source": [
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yykpyd4CLhD"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries from Keras for building a neural network\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "\n",
        "# -----------------------------\n",
        "# Building the Neural Network model\n",
        "# -----------------------------\n",
        "\n",
        "# Initialize a Sequential model (a linear stack of layers)\n",
        "classifier = Sequential()\n",
        "\n",
        "# Input layer: expects input vectors of size 30 (since dataset has 30 features)\n",
        "classifier.add(Input(shape=(30,)))\n",
        "\n",
        "# First hidden layer:\n",
        "# - Dense = fully connected layer with 16 neurons\n",
        "# - kernel_initializer='uniform' initializes weights randomly with a uniform distribution\n",
        "# - activation='relu' introduces non-linearity (ReLU is common in hidden layers)\n",
        "classifier.add(Dense(units=16, kernel_initializer='uniform', activation='relu'))\n",
        "\n",
        "# Dropout layer:\n",
        "# - Randomly drops 50% of the neurons during training\n",
        "# - Helps prevent overfitting (important since dataset is relatively small, ~569 rows)\n",
        "classifier.add(Dropout(rate=0.5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGsk-pGvCLrW"
      },
      "outputs": [],
      "source": [
        "# adding the second hidden layer\n",
        "classifier.add(Dense(units=16, kernel_initializer='uniform', activation='relu'))\n",
        "classifier.add(Dropout(rate=0.5))  # probably better to use 0.2–0.5 in practice\n",
        "\n",
        "# adding the output layer\n",
        "classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSfEeEUvCL1L"
      },
      "outputs": [],
      "source": [
        "# Compile the neural network model\n",
        "classifier.compile(\n",
        "    optimizer=\"Adam\",              # Adam optimizer: adaptive learning rate, works well for most problems\n",
        "    loss='binary_crossentropy',    # Loss function for binary classification (Malignant vs Benign)\n",
        "    metrics=['accuracy']           # Track accuracy during training and testing\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CesBCYmfCL9X"
      },
      "outputs": [],
      "source": [
        "# Train the neural network on the training data\n",
        "classifier.fit(\n",
        "    X_train,        # feature inputs (30 tumor measurements per sample)\n",
        "    y_train,        # target labels (0 = Benign, 1 = Malignant)\n",
        "    batch_size=100, # number of samples processed before model updates weights once\n",
        "    epochs=150      # number of complete passes through the training dataset\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eihTUHJFMlb"
      },
      "outputs": [],
      "source": [
        "X_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o1fRDIRGFRF"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Predicting the Test set results\n",
        "# -----------------------------\n",
        "\n",
        "# Use the trained classifier to predict probabilities on the test set\n",
        "# Each output is a value between 0 and 1 (since binary classification with sigmoid output)\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Convert probabilities into binary predictions:\n",
        "# If probability > 0.5 → classify as 1 (Malignant)\n",
        "# If probability <= 0.5 → classify as 0 (Benign)\n",
        "y_pred = (y_pred > 0.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgyO0aOaGIST"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Making the Confusion Matrix\n",
        "# -----------------------------\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Create a confusion matrix to evaluate the classifier's performance\n",
        "# Compares actual labels (y_test) with predicted labels (y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOjJ25VGGR-9"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Visualizing the Confusion Matrix\n",
        "# -----------------------------\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "# 'annot=True' writes the numbers (TN, FP, FN, TP) inside the heatmap cells\n",
        "sns.heatmap(cm, annot=True)\n",
        "\n",
        "# Save the plotted heatmap as an image file 'h.png'\n",
        "# This lets you keep a visual copy of your confusion matrix\n",
        "plt.savefig('h.png')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}